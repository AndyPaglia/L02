# L02
L02 - CapstoneProject - Can we detect the Fake  Music generator from the  description of the music?

CODE WORKFLOW:
To make this code work, the things to check are the paths to the folders.
The code takes the files from a folder called L02CapstoneProject, that is saved on
GoogleDrive. This is why we have to mount GoogleDrive in GoogleColab in every
code.
The workflow to follow, starting from the full FakeMusicCaps dataset, is this one:
1. DataSetReductionFakeMusicCaps: used to reduce the dataset folder
FakeMusicCaps for running the next code without problems inherent to
Google Colabâ€™s limitations for runtime and GPU use. We choose to extract
2500 files for each subfolder (for each class) of the FakeMusicCaps folder.
2. CheckNumberOfElementsInAFolder: used to check if the elements in the
folder created with the above function are in the number we want.
3. Step1_Captioning: from FakeMusicCaps2500, where we have audio files in
wav form, we create their textual captions using lp-music-caps.
4. Extract10secondsSunoCaps: extracting only the first 10s of SunoCaps, since
the other subfolders contain only 10s audio files. We give time coherence
from one caption to the other.
5. Step2_TextAttribution: Apply text classification to detect which generator was
used to create the audio. We do text classification using BERT model.
6. Step3.a_OpenSetThreshold: This code implements a text classification
pipeline using BERT to identify music captions generated by different models,
with an open-set recognition mechanism to detect unknown classes (here
"SunoCaps"). The classification boundary is defined by a threshold.
7. Step3.b_OpenSetSVM: This code implements a text classification pipeline
using BERT to identify music captions generated by different models,
incorporating an open-set recognition mechanism based on Support Vector
Machines (SVM) to detect unknown classes (here "SunoCaps")
